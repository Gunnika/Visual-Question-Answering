# Visual-Question-Answering

## Datasets : 
https://visualqa.org/download.html,

https://arxiv.org/abs/1905.13648,

https://tryolabs.com/blog/2018/03/01/introduction-to-visual-question-answering/

https://iamaaditya.github.io/2016/04/visual_question_answering_demo_notebook

https://paperswithcode.com/task/visual-question-answering

https://vqa.cloudcv.org/

## Dataset we are using:

### VQA v2.0
It consists of : 

- Real
  - 82,783 MS COCO training images, 40,504 MS COCO validation images and 81,434 MS COCO testing images (images are obtained from [MS COCO website] (http://mscoco.org/dataset/#download))
  - 443,757 questions for training, 214,354 questions for validation and 447,793 questions for testing
  - 4,437,570 answers for training and 2,143,540 answers for validation (10 per question)
  
- There is only one type of task - 
Open-ended task

## Research paper we are referring to: 
https://arxiv.org/abs/1704.03162

PDF : https://arxiv.org/pdf/1704.03162.pdf

## Base Code:
https://github.com/Cyanogenoid/pytorch-vqa

Improvements over this code : https://github.com/Cyanogenoid/vqa-counting/tree/master/vqa-v2


