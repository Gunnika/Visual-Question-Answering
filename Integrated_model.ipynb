{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Reshape, Activation, Dropout\n",
    "from keras.layers import LSTM, Dense, Add, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_features = np.load('img_vectors_sample.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913, 512)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_features = np.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VQA_MODEL():\n",
    "    image_feature_size          = 4096\n",
    "    word_feature_size           = 300\n",
    "    number_of_LSTM              = 3\n",
    "    number_of_hidden_units_LSTM = 512\n",
    "    max_length_questions        = 30\n",
    "    number_of_dense_layers      = 3\n",
    "    number_of_hidden_units      = 1024\n",
    "    activation_function         = 'tanh'\n",
    "    dropout_pct                 = 0.5\n",
    "\n",
    "\n",
    "    # Image model\n",
    "    model_image = Sequential()\n",
    "    model_image.add(Reshape((image_feature_size,), input_shape=(image_feature_size,)))\n",
    "\n",
    "    # Language Model\n",
    "    model_language = Sequential()\n",
    "    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=True, input_shape=(max_length_questions, word_feature_size)))\n",
    "    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=True))\n",
    "    model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=False))\n",
    "\n",
    "    # combined model\n",
    "    model = Sequential()\n",
    "    model.add(Concatenate([model_language, model_image], concat_axis=1))\n",
    "\n",
    "    for _ in xrange(number_of_dense_layers):\n",
    "        model.add(Dense(number_of_hidden_units, kernel_initializer='uniform'))\n",
    "        model.add(Activation(activation_function))\n",
    "        model.add(Dropout(dropout_pct))\n",
    "\n",
    "    model.add(Dense(1000))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def get_arguments():\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # model\n",
    "    parser.add_argument('-model'                  , type=str   , default='simple_mlp')\n",
    "    parser.add_argument('-num_hidden_units_mlp'   , type=int   , default=1024)\n",
    "    parser.add_argument('-num_hidden_units_lstm'  , type=int   , default=512)\n",
    "    parser.add_argument('-num_hidden_layers_mlp'  , type=int   , default=3)\n",
    "    parser.add_argument('-num_hidden_layers_lstm' , type=int   , default=1)\n",
    "    parser.add_argument('-dropout'                , type=float , default=0.5)\n",
    "    parser.add_argument('-activation_1'           , type=str   , default='tanh')\n",
    "    parser.add_argument('-activation_2'           , type=str   , default='relu')\n",
    "\n",
    "    # training\n",
    "    parser.add_argument('-seed'                   , type=int   , default=1337)\n",
    "    parser.add_argument('-optimizer'              , type=str   , default='rmsprop')\n",
    "    parser.add_argument('-nb_epoch'               , type=int   , default=300)\n",
    "    parser.add_argument('-nb_iter'                , type=int   , default=200000)\n",
    "    parser.add_argument('-model_save_interval'    , type=int   , default=19)\n",
    "    parser.add_argument('-batch_size'             , type=int   , default=128)\n",
    "\n",
    "    # language features\n",
    "    parser.add_argument('-word_vector'            , type=str   , default='glove')\n",
    "    parser.add_argument('-word_emb_dim'           , type=int   , default=300)\n",
    "    parser.add_argument('-vocabulary_size'        , type=int   , default=12603)\n",
    "    parser.add_argument('-max_ques_length'        , type=int   , default=26)\n",
    "    parser.add_argument('-data_type'              , type=str   , default='TRAIN')\n",
    "\n",
    "    # image features\n",
    "    parser.add_argument('-img_vec_dim'            , type=int   , default=2048)\n",
    "    parser.add_argument('-img_features'           , type=str   , default='resnet')\n",
    "    parser.add_argument('-img_normalize'          , type=int   , default=0)\n",
    "\n",
    "    # evaluations\n",
    "    parser.add_argument('-nb_classes'             , type=int   , default=1000)\n",
    "    parser.add_argument('-class_activation'       , type=str   , default='softmax')\n",
    "    parser.add_argument('-loss'                   , type=str   , default='categorical_crossentropy')\n",
    "    parser.add_argument('-save_folder'            , type=str   , default='')\n",
    "\n",
    "    # data\n",
    "    parser.add_argument('-ans_file'               , type=str   , default='data/val_all_answers_dict.json')\n",
    "    parser.add_argument('-input_json'             , type=str   , default='data/data_prepro.json')\n",
    "    parser.add_argument('-input_img_h5'           , type=str   , default='data/data_img.h5')\n",
    "    parser.add_argument('-input_ques_h5'          , type=str   , default='data/data_prepro.h5')\n",
    "\n",
    "\n",
    "    return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py  as hf\n",
    "import json\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "def get_train_data(args):\n",
    "\n",
    "#     dataset = {}\n",
    "#     train_data = {}\n",
    "#     # load json file\n",
    "#     print('loading json file...')\n",
    "#     with open(args.input_json) as data_file:\n",
    "#         data = json.load(data_file)\n",
    "#     for key in data.keys():\n",
    "#         dataset[key] = data[key]\n",
    "\n",
    "    # load image feature\n",
    "    print('loading image feature...')\n",
    "    with h5py.File(args.input_img_h5,'r') as hf:\n",
    "        # -----0~82459------\n",
    "        tem = hf.get('images_train')\n",
    "        img_feature = np.array(tem)\n",
    "        \n",
    "    # load h5 file\n",
    "    print('loading h5 file...')\n",
    "    with h5py.File(args.input_ques_h5,'r') as hf:\n",
    "        # total number of training data is 215375\n",
    "        # question is (26, )\n",
    "        tem = hf.get('ques_train')\n",
    "        train_data['question'] = np.array(tem)\n",
    "        # max length is 23\n",
    "        tem = hf.get('ques_length_train')\n",
    "        train_data['length_q'] = np.array(tem)\n",
    "        # total 82460 img\n",
    "        #-----1~82460-----\n",
    "        tem = hf.get('img_pos_train')\n",
    "    # convert into 0~82459\n",
    "        train_data['img_list'] = np.array(tem)-1\n",
    "        # answer is 1~1000\n",
    "        tem = hf.get('answers')\n",
    "        train_data['answers'] = np.array(tem)-1\n",
    "\n",
    "    print('Normalizing image feature')\n",
    "    if img_norm:\n",
    "        tem = np.sqrt(np.sum(np.multiply(img_feature, img_feature)))\n",
    "        img_feature = np.divide(img_feature, np.tile(tem,(1,args.img_vec_dim)))\n",
    "\n",
    "    return dataset, img_feature, train_data\n",
    "\n",
    "def get_data_test(args):\n",
    "    dataset = {}\n",
    "    test_data = {}\n",
    "    # load json file\n",
    "    print('loading json file...')\n",
    "    with open(args.input_json) as data_file:\n",
    "        data = json.load(data_file)\n",
    "    for key in data.keys():\n",
    "        dataset[key] = data[key]\n",
    "\n",
    "    # load image feature\n",
    "    print('loading image feature...')\n",
    "    img_feature = np.load('img_vectors_sample.npy')\n",
    "    \n",
    "    # load h5 file\n",
    "    print('loading h5 file...')\n",
    "    with h5py.File(args.input_ques_h5,'r') as hf:\n",
    "        # total number of training data is 215375\n",
    "        # question is (26, )\n",
    "        tem = hf.get('ques_test')\n",
    "        test_data['question'] = np.array(tem)\n",
    "        # max length is 23\n",
    "        tem = hf.get('ques_length_test')\n",
    "        test_data['length_q'] = np.array(tem)\n",
    "        # total 82460 img\n",
    "        # -----1~82460-----\n",
    "        tem = hf.get('img_pos_test')\n",
    "        # convert into 0~82459\n",
    "        test_data['img_list'] = np.array(tem)-1\n",
    "        # quiestion id\n",
    "        tem = hf.get('question_id_test')\n",
    "        test_data['ques_id'] = np.array(tem)\n",
    "    # MC_answer_test\n",
    "    tem = hf.get('MC_ans_test')\n",
    "    test_data['MC_ans_test'] = np.array(tem)\n",
    "\n",
    "#     print('Normalizing image feature')\n",
    "#     if img_norm:\n",
    "#         tem =  np.sqrt(np.sum(np.multiply(img_feature, img_feature)))\n",
    "#         img_feature = np.divide(img_feature, np.tile(tem,(1,args.img_vec_dim)))\n",
    "\n",
    "\n",
    "    # make sure the ans_file is provided\n",
    "    nb_data_test = len(test_data[u'question'])\n",
    "    val_all_answers_dict = json.load(open(args.ans_file))\n",
    "    val_answers = np.zeros(nb_data_test, dtype=np.int32)\n",
    "\n",
    "    ans_to_ix = {v: k for k, v in dataset[u'ix_to_ans'].items()}\n",
    "    count_of_not_found = 0\n",
    "    for i in xrange(nb_data_test):\n",
    "        qid = test_data[u'ques_id'][i]\n",
    "        try : \n",
    "            val_ans_ix =int(ans_to_ix[most_common(val_all_answers_dict[str(qid)])]) -1\n",
    "        except KeyError:\n",
    "            count_of_not_found += 1\n",
    "            val_ans_ix = 480\n",
    "        val_answers[i] = val_ans_ix\n",
    "    print(\"Beware: \" + str(count_of_not_found) + \" number of val answers are not really correct\")\n",
    "\n",
    "    return dataset, img_feature, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# custom\n",
    "from utils.get_data import get_test_data, get_train_data\n",
    "from utils.arguments import get_arguments\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    args = get_arguments()\n",
    "    print(args)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "\n",
    "    dataset, train_img_feature, train_data = get_train_data(args)\n",
    "    dataset, test_img_feature,  test_data, val_answers = get_test_data(args)\n",
    "\n",
    "    train_X = [train_data[u'question'], train_img_feature]\n",
    "    train_Y = np_utils.to_categorical(train_data[u'answers'], args.nb_classes)\n",
    "\n",
    "    test_X = [test_data[u'question'], test_img_feature]\n",
    "    test_Y = np_utils.to_categorical(val_answers, args.nb_classes)\n",
    "\n",
    "\n",
    "    model_name = importlib.import_module(\"models.\"+args.model)\n",
    "    model = model_name.model(args)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=args.optimizer, metrics=['accuracy'])\n",
    "    model.summary() # prints model layers with weights\n",
    "\n",
    "    history = model.fit(train_X, train_Y, batch_size = args.batch_size, nb_epoch=args.nb_epoch, validation_data=(test_X, test_Y))\n",
    "\n",
    "    return history.history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VQA_weights_file_name   = 'models/VQA/VQA_MODEL_WEIGHTS.hdf5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VQA_model(VQA_weights_file_name):\n",
    "    ''' Given the VQA model and its weights, compiles and returns the model '''\n",
    "\n",
    "    from models.VQA.VQA import VQA_MODEL\n",
    "    vqa_model = VQA_MODEL()\n",
    "    vqa_model.load_weights(VQA_weights_file_name)\n",
    "\n",
    "    vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_model = get_VQA_model(VQA_weights_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_model.predict([question_features, image_features])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
