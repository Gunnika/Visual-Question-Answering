{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, RepeatVector, Masking, TimeDistributed\n",
    "from tensorflow. keras.utils import plot_model\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image ID</th>\n",
       "      <th>Image Vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458752</td>\n",
       "      <td>[4.80218083e-02 9.07960236e-01 1.19339478e+00 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262146</td>\n",
       "      <td>[7.87118018e-01 2.44311482e-01 6.21284246e-02 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>524291</td>\n",
       "      <td>[1.1351358  0.643366   1.2675045  0.10811964 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>393221</td>\n",
       "      <td>[2.51035309e+00 2.11022353e+00 4.24877226e-01 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>393223</td>\n",
       "      <td>[1.42517245e+00 4.25842971e-01 9.63381290e-01 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Image ID                                       Image Vector\n",
       "0    458752  [4.80218083e-02 9.07960236e-01 1.19339478e+00 ...\n",
       "1    262146  [7.87118018e-01 2.44311482e-01 6.21284246e-02 ...\n",
       "2    524291  [1.1351358  0.643366   1.2675045  0.10811964 1...\n",
       "3    393221  [2.51035309e+00 2.11022353e+00 4.24877226e-01 ...\n",
       "4    393223  [1.42517245e+00 4.25842971e-01 9.63381290e-01 ..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=pd.read_csv(r\"Files/image_vectors_with_ids.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>458752</td>\n",
       "      <td>What is this photo taken looking through?</td>\n",
       "      <td>458752000</td>\n",
       "      <td>(((tf.Tensor(-0.1294253, shape=(), dtype=float...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>458752</td>\n",
       "      <td>What position is this man playing?</td>\n",
       "      <td>458752001</td>\n",
       "      <td>(((tf.Tensor(-0.11989767, shape=(), dtype=floa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>458752</td>\n",
       "      <td>What color is the players shirt?</td>\n",
       "      <td>458752002</td>\n",
       "      <td>(((tf.Tensor(-0.085942656, shape=(), dtype=flo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>458752</td>\n",
       "      <td>Is this man a professional baseball player?</td>\n",
       "      <td>458752003</td>\n",
       "      <td>(((tf.Tensor(-0.11925976, shape=(), dtype=floa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>262146</td>\n",
       "      <td>What color is the snow?</td>\n",
       "      <td>262146000</td>\n",
       "      <td>(((tf.Tensor(-0.04078256, shape=(), dtype=floa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   image_id                                     question  question_id  \\\n",
       "0    458752    What is this photo taken looking through?    458752000   \n",
       "1    458752           What position is this man playing?    458752001   \n",
       "2    458752             What color is the players shirt?    458752002   \n",
       "3    458752  Is this man a professional baseball player?    458752003   \n",
       "4    262146                      What color is the snow?    262146000   \n",
       "\n",
       "                                                 vec  \n",
       "0  (((tf.Tensor(-0.1294253, shape=(), dtype=float...  \n",
       "1  (((tf.Tensor(-0.11989767, shape=(), dtype=floa...  \n",
       "2  (((tf.Tensor(-0.085942656, shape=(), dtype=flo...  \n",
       "3  (((tf.Tensor(-0.11925976, shape=(), dtype=floa...  \n",
       "4  (((tf.Tensor(-0.04078256, shape=(), dtype=floa...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questionsdf = pickle.load(open(r\"C:\\Users\\PIYUSH\\Downloads\\bert_small.pkl\", \"rb\"))\n",
    "questionsdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_features = questionsdf['vec'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 12, 768])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_features[18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 10, 768])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(question_features.shape[0]):\n",
    "    a=question_features[i].shape[1]\n",
    "    question_features[i]=tf.reshape(question_features[i],[1*a*768])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7680])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_tensor(question,image):\n",
    "    t=tf.concat([question,image], 0)\n",
    "    s=11600-t.shape[0]\n",
    "    paddings = tf.constant([[0, s,]])\n",
    "    t=tf.pad(t, paddings, \"CONSTANT\")\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 34, 32, ..., 37, 26, 59])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = []\n",
    "for quest in questionsdf['question']:\n",
    "    lens.append(len(quest))\n",
    "question_length = np.array(lens)\n",
    "question_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9935,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_length.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " 24,\n",
       " 29,\n",
       " 33,\n",
       " 35,\n",
       " 48,\n",
       " 60,\n",
       " 63,\n",
       " 70,\n",
       " 71,\n",
       " 76,\n",
       " 77,\n",
       " 80,\n",
       " 88,\n",
       " 91,\n",
       " 93,\n",
       " 108,\n",
       " 109,\n",
       " 112,\n",
       " 126,\n",
       " 137,\n",
       " 141,\n",
       " 143,\n",
       " 148,\n",
       " 153,\n",
       " 164,\n",
       " 193,\n",
       " 200,\n",
       " 246,\n",
       " 249,\n",
       " 259,\n",
       " 306,\n",
       " 307,\n",
       " 308,\n",
       " 311,\n",
       " 314,\n",
       " 320,\n",
       " 321,\n",
       " 325,\n",
       " 331,\n",
       " 367,\n",
       " 369,\n",
       " 381,\n",
       " 383,\n",
       " 388,\n",
       " 393,\n",
       " 403,\n",
       " 418,\n",
       " 430,\n",
       " 435,\n",
       " 437,\n",
       " 442,\n",
       " 449,\n",
       " 470,\n",
       " 507,\n",
       " 509,\n",
       " 513,\n",
       " 528,\n",
       " 530,\n",
       " 531,\n",
       " 539,\n",
       " 541,\n",
       " 561,\n",
       " 571,\n",
       " 574,\n",
       " 580,\n",
       " 583,\n",
       " 594,\n",
       " 596,\n",
       " 604,\n",
       " 611,\n",
       " 619,\n",
       " 624,\n",
       " 628,\n",
       " 633,\n",
       " 642,\n",
       " 649,\n",
       " 655,\n",
       " 658,\n",
       " 672,\n",
       " 680,\n",
       " 689,\n",
       " 713,\n",
       " 715,\n",
       " 721,\n",
       " 722,\n",
       " 734,\n",
       " 753,\n",
       " 761,\n",
       " 780,\n",
       " 789,\n",
       " 794,\n",
       " 796,\n",
       " 824,\n",
       " 827,\n",
       " 852,\n",
       " 881,\n",
       " 896,\n",
       " 901,\n",
       " 907,\n",
       " 908,\n",
       " 912,\n",
       " 924,\n",
       " 926,\n",
       " 933,\n",
       " 940,\n",
       " 954,\n",
       " 964,\n",
       " 981,\n",
       " 983,\n",
       " 995,\n",
       " 1005,\n",
       " 1013,\n",
       " 1024,\n",
       " 1035,\n",
       " 1058,\n",
       " 1071,\n",
       " 1083,\n",
       " 1089,\n",
       " 1098,\n",
       " 1101,\n",
       " 1106,\n",
       " 1107,\n",
       " 1110,\n",
       " 1121,\n",
       " 1138,\n",
       " 1154,\n",
       " 1165,\n",
       " 1167,\n",
       " 1182,\n",
       " 1199,\n",
       " 1203,\n",
       " 1215,\n",
       " 1223,\n",
       " 1231,\n",
       " 1263,\n",
       " 1270,\n",
       " 1281,\n",
       " 1294,\n",
       " 1305,\n",
       " 1306,\n",
       " 1307,\n",
       " 1310,\n",
       " 1314,\n",
       " 1329,\n",
       " 1331,\n",
       " 1354,\n",
       " 1359,\n",
       " 1365,\n",
       " 1374,\n",
       " 1380,\n",
       " 1385,\n",
       " 1389,\n",
       " 1391,\n",
       " 1396,\n",
       " 1400,\n",
       " 1402,\n",
       " 1406,\n",
       " 1407,\n",
       " 1430,\n",
       " 1452,\n",
       " 1454,\n",
       " 1471,\n",
       " 1487,\n",
       " 1495,\n",
       " 1496,\n",
       " 1500,\n",
       " 1504,\n",
       " 1506,\n",
       " 1509,\n",
       " 1514,\n",
       " 1517,\n",
       " 1521,\n",
       " 1525,\n",
       " 1535,\n",
       " 1547,\n",
       " 1557,\n",
       " 1561,\n",
       " 1568,\n",
       " 1578,\n",
       " 1579,\n",
       " 1585,\n",
       " 1588,\n",
       " 1610,\n",
       " 1621,\n",
       " 1624,\n",
       " 1636,\n",
       " 1638,\n",
       " 1644,\n",
       " 1669,\n",
       " 1673,\n",
       " 1680,\n",
       " 1696,\n",
       " 1711,\n",
       " 1719,\n",
       " 1736,\n",
       " 1761,\n",
       " 1763,\n",
       " 1770,\n",
       " 1773,\n",
       " 1776,\n",
       " 1780,\n",
       " 1785,\n",
       " 1789,\n",
       " 1791,\n",
       " 1803,\n",
       " 1810,\n",
       " 1812,\n",
       " 1814,\n",
       " 1821,\n",
       " 1836,\n",
       " 1876,\n",
       " 1887,\n",
       " 1894,\n",
       " 1901,\n",
       " 1905,\n",
       " 1906,\n",
       " 1910,\n",
       " 1911,\n",
       " 1914,\n",
       " 1923,\n",
       " 1941,\n",
       " 1942,\n",
       " 1965,\n",
       " 1993,\n",
       " 1998,\n",
       " 2023,\n",
       " 2055,\n",
       " 2065,\n",
       " 2071,\n",
       " 2082,\n",
       " 2088,\n",
       " 2113,\n",
       " 2134,\n",
       " 2147,\n",
       " 2149,\n",
       " 2183,\n",
       " 2210,\n",
       " 2216,\n",
       " 2231,\n",
       " 2257,\n",
       " 2275,\n",
       " 2277,\n",
       " 2278,\n",
       " 2279,\n",
       " 2280,\n",
       " 2282,\n",
       " 2295,\n",
       " 2336,\n",
       " 2341,\n",
       " 2348,\n",
       " 2371,\n",
       " 2373,\n",
       " 2376,\n",
       " 2388,\n",
       " 2399,\n",
       " 2401,\n",
       " 2410,\n",
       " 2414,\n",
       " 2428,\n",
       " 2443,\n",
       " 2447,\n",
       " 2465,\n",
       " 2469,\n",
       " 2495,\n",
       " 2497,\n",
       " 2530,\n",
       " 2535,\n",
       " 2542,\n",
       " 2543,\n",
       " 2544,\n",
       " 2554,\n",
       " 2559,\n",
       " 2562,\n",
       " 2566,\n",
       " 2569,\n",
       " 2574,\n",
       " 2582,\n",
       " 2584,\n",
       " 2590,\n",
       " 2605,\n",
       " 2607,\n",
       " 2617,\n",
       " 2618,\n",
       " 2622,\n",
       " 2623,\n",
       " 2643,\n",
       " 2663,\n",
       " 2686,\n",
       " 2692,\n",
       " 2696,\n",
       " 2702,\n",
       " 2731,\n",
       " 2741,\n",
       " 2751,\n",
       " 2753,\n",
       " 2754,\n",
       " 2757,\n",
       " 2769,\n",
       " 2773,\n",
       " 2775,\n",
       " 2781,\n",
       " 2841,\n",
       " 2842,\n",
       " 2848,\n",
       " 2859,\n",
       " 2885,\n",
       " 2891,\n",
       " 2901,\n",
       " 2906,\n",
       " 2930,\n",
       " 4375,\n",
       " 4376,\n",
       " 4409,\n",
       " 4440,\n",
       " 4461,\n",
       " 8746,\n",
       " 20089,\n",
       " 21825,\n",
       " 21894,\n",
       " 21914,\n",
       " 21925,\n",
       " 21944,\n",
       " 21963,\n",
       " 21982,\n",
       " 21990,\n",
       " 22153,\n",
       " 22179,\n",
       " 22193,\n",
       " 22197,\n",
       " 22221,\n",
       " 22222,\n",
       " 22267,\n",
       " 22280,\n",
       " 22308,\n",
       " 25141,\n",
       " 26226,\n",
       " 30584,\n",
       " 30618,\n",
       " 33176,\n",
       " 34932,\n",
       " 35007,\n",
       " 37185,\n",
       " 39287,\n",
       " 39370,\n",
       " 43696,\n",
       " 43772,\n",
       " 43779,\n",
       " 43892,\n",
       " 43935,\n",
       " 44046,\n",
       " 44116,\n",
       " 44146,\n",
       " 44159,\n",
       " 48046,\n",
       " 49356,\n",
       " 51503,\n",
       " 53930,\n",
       " 56771,\n",
       " 61143,\n",
       " 62851,\n",
       " 65715,\n",
       " 65736,\n",
       " 65834,\n",
       " 65904,\n",
       " 66002,\n",
       " 66013,\n",
       " 69928,\n",
       " 74252,\n",
       " 78651,\n",
       " 80584,\n",
       " 82997,\n",
       " 83101,\n",
       " 84682,\n",
       " 87512,\n",
       " 87517,\n",
       " 87524,\n",
       " 87553,\n",
       " 87632,\n",
       " 87661,\n",
       " 87725,\n",
       " 95158,\n",
       " 96066,\n",
       " 98719,\n",
       " 100412,\n",
       " 100559,\n",
       " 101430,\n",
       " 104816,\n",
       " 104843,\n",
       " 107715,\n",
       " 109147,\n",
       " 109155,\n",
       " 109276,\n",
       " 109323,\n",
       " 109398,\n",
       " 109408,\n",
       " 109552,\n",
       " 109621,\n",
       " 109699,\n",
       " 109985,\n",
       " 112496,\n",
       " 113512,\n",
       " 114352,\n",
       " 117938,\n",
       " 126650,\n",
       " 131010,\n",
       " 131073,\n",
       " 131074,\n",
       " 131083,\n",
       " 131086,\n",
       " 131092,\n",
       " 131100,\n",
       " 131112,\n",
       " 131117,\n",
       " 131125,\n",
       " 131126,\n",
       " 131127,\n",
       " 131132,\n",
       " 131159,\n",
       " 131171,\n",
       " 131173,\n",
       " 131189,\n",
       " 131196,\n",
       " 131207,\n",
       " 131214,\n",
       " 131224,\n",
       " 131244,\n",
       " 131276,\n",
       " 131278,\n",
       " 131298,\n",
       " 131299,\n",
       " 131311,\n",
       " 131314,\n",
       " 131322,\n",
       " 131329,\n",
       " 131338,\n",
       " 131341,\n",
       " 131350,\n",
       " 131351,\n",
       " 131363,\n",
       " 131372,\n",
       " 131373,\n",
       " 131375,\n",
       " 131387,\n",
       " 131399,\n",
       " 131414,\n",
       " 131418,\n",
       " 131426,\n",
       " 131433,\n",
       " 131448,\n",
       " 131449,\n",
       " 131464,\n",
       " 131466,\n",
       " 131469,\n",
       " 131484,\n",
       " 131485,\n",
       " 131486,\n",
       " 131497,\n",
       " 131508,\n",
       " 131510,\n",
       " 131521,\n",
       " 131523,\n",
       " 131563,\n",
       " 131564,\n",
       " 131578,\n",
       " 131588,\n",
       " 131594,\n",
       " 131612,\n",
       " 131620,\n",
       " 131665,\n",
       " 131673,\n",
       " 131677,\n",
       " 131695,\n",
       " 131702,\n",
       " 131720,\n",
       " 131734,\n",
       " 131741,\n",
       " 131746,\n",
       " 131763,\n",
       " 131779,\n",
       " 131783,\n",
       " 131815,\n",
       " 131832,\n",
       " 131839,\n",
       " 131857,\n",
       " 131864,\n",
       " 131878,\n",
       " 131893,\n",
       " 131901,\n",
       " 131908,\n",
       " 131910,\n",
       " 131917,\n",
       " 131936,\n",
       " 131951,\n",
       " 131960,\n",
       " 131966,\n",
       " 131975,\n",
       " 131978,\n",
       " 131999,\n",
       " 132001,\n",
       " 132014,\n",
       " 132018,\n",
       " 132076,\n",
       " 132080,\n",
       " 132093,\n",
       " 132098,\n",
       " 132105,\n",
       " 132113,\n",
       " 132119,\n",
       " 132122,\n",
       " 132136,\n",
       " 132138,\n",
       " 132140,\n",
       " 132146,\n",
       " 132164,\n",
       " 132169,\n",
       " 132187,\n",
       " 132195,\n",
       " 132211,\n",
       " 132216,\n",
       " 132248,\n",
       " 132257,\n",
       " 132261,\n",
       " 132264,\n",
       " 132289,\n",
       " 132293,\n",
       " 132296,\n",
       " 132297,\n",
       " 132298,\n",
       " 132302,\n",
       " 132305,\n",
       " 132309,\n",
       " 132329,\n",
       " 132330,\n",
       " 132384,\n",
       " 132385,\n",
       " 132393,\n",
       " 132404,\n",
       " 132411,\n",
       " 132419,\n",
       " 132429,\n",
       " 132453,\n",
       " 132469,\n",
       " 132475,\n",
       " 132484,\n",
       " 132493,\n",
       " 132494,\n",
       " 132496,\n",
       " 132499,\n",
       " 132514,\n",
       " 132515,\n",
       " 132516,\n",
       " 132519,\n",
       " 132520,\n",
       " 132522,\n",
       " 132527,\n",
       " 132528,\n",
       " 132530,\n",
       " 132532,\n",
       " 132548,\n",
       " 132551,\n",
       " 132563,\n",
       " 132564,\n",
       " 132570,\n",
       " 132573,\n",
       " 132575,\n",
       " 132590,\n",
       " 132616,\n",
       " 132620,\n",
       " 132625,\n",
       " 132645,\n",
       " 132653,\n",
       " 132655,\n",
       " 132668,\n",
       " 132669,\n",
       " 132697,\n",
       " 132709,\n",
       " 132724,\n",
       " 132749,\n",
       " 132757,\n",
       " 132759,\n",
       " 132767,\n",
       " 132777,\n",
       " 132788,\n",
       " 132792,\n",
       " 132794,\n",
       " 132797,\n",
       " 132815,\n",
       " 132837,\n",
       " 132840,\n",
       " 132846,\n",
       " 132849,\n",
       " 132856,\n",
       " 132873,\n",
       " 132877,\n",
       " 132887,\n",
       " 132901,\n",
       " 132907,\n",
       " 132912,\n",
       " 132934,\n",
       " 132943,\n",
       " 132953,\n",
       " 132958,\n",
       " 132963,\n",
       " 132971,\n",
       " 132972,\n",
       " 132990,\n",
       " 132992,\n",
       " 132996,\n",
       " 133003,\n",
       " 133012,\n",
       " 133029,\n",
       " 133062,\n",
       " 133109,\n",
       " 133112,\n",
       " 133113,\n",
       " 133114,\n",
       " 133118,\n",
       " 133136,\n",
       " 133144,\n",
       " 133150,\n",
       " 133151,\n",
       " 133165,\n",
       " 133166,\n",
       " 133174,\n",
       " 133182,\n",
       " 133193,\n",
       " 133199,\n",
       " 133207,\n",
       " 133226,\n",
       " 133228,\n",
       " 133233,\n",
       " 133246,\n",
       " 133256,\n",
       " 133260,\n",
       " 133272,\n",
       " 133277,\n",
       " 133293,\n",
       " 133294,\n",
       " 133297,\n",
       " 133307,\n",
       " 133313,\n",
       " 133314,\n",
       " 133330,\n",
       " 133368,\n",
       " 133383,\n",
       " 133402,\n",
       " 133409,\n",
       " 133425,\n",
       " 133427,\n",
       " 133435,\n",
       " 133452,\n",
       " 133455,\n",
       " 133461,\n",
       " 133466,\n",
       " 133481,\n",
       " 133485,\n",
       " 133489,\n",
       " 133491,\n",
       " 133495,\n",
       " 133504,\n",
       " 133507,\n",
       " 133509,\n",
       " 133539,\n",
       " 133548,\n",
       " 133553,\n",
       " 133564,\n",
       " 133575,\n",
       " 133593,\n",
       " 133633,\n",
       " 133640,\n",
       " 133649,\n",
       " 133653,\n",
       " 133659,\n",
       " 133671,\n",
       " 133687,\n",
       " 133696,\n",
       " 133721,\n",
       " 133726,\n",
       " 133729,\n",
       " 133730,\n",
       " 133738,\n",
       " 133740,\n",
       " 133765,\n",
       " 133787,\n",
       " 133790,\n",
       " 133830,\n",
       " 133834,\n",
       " 133836,\n",
       " 133838,\n",
       " 133844,\n",
       " 133849,\n",
       " 133878,\n",
       " 133882,\n",
       " 133884,\n",
       " 133904,\n",
       " 133907,\n",
       " 133908,\n",
       " 133911,\n",
       " 133936,\n",
       " 133989,\n",
       " 133993,\n",
       " 134778,\n",
       " 135331,\n",
       " 135396,\n",
       " 136184,\n",
       " 137044,\n",
       " 137917,\n",
       " 137930,\n",
       " 139104,\n",
       " 142297,\n",
       " 144083,\n",
       " 144087,\n",
       " 151974,\n",
       " 152973,\n",
       " 153030,\n",
       " 153038,\n",
       " 153122,\n",
       " 153127,\n",
       " 153396,\n",
       " 154506,\n",
       " 155378,\n",
       " 157124,\n",
       " 157152,\n",
       " 157189,\n",
       " 157204,\n",
       " 157218,\n",
       " 165858,\n",
       " 165926,\n",
       " 170234,\n",
       " 170272,\n",
       " 170294,\n",
       " 171104,\n",
       " 174604,\n",
       " 174676,\n",
       " 174963,\n",
       " 175016,\n",
       " 175046,\n",
       " 175071,\n",
       " 175087,\n",
       " 175111,\n",
       " 175128,\n",
       " 175179,\n",
       " 175184,\n",
       " 175209,\n",
       " 175237,\n",
       " 175249,\n",
       " 177898,\n",
       " 178956,\n",
       " 181565,\n",
       " 183400,\n",
       " 192052,\n",
       " 192926,\n",
       " 194668,\n",
       " 194670,\n",
       " 196662,\n",
       " 196687,\n",
       " 196776,\n",
       " 197000,\n",
       " 197056,\n",
       " 197067,\n",
       " 199027,\n",
       " 200781,\n",
       " 200800,\n",
       " 202522,\n",
       " 202530,\n",
       " 205158,\n",
       " 209573,\n",
       " 209590,\n",
       " 212640,\n",
       " 212997,\n",
       " 213862,\n",
       " 218507,\n",
       " 218556,\n",
       " 218588,\n",
       " 218594,\n",
       " 218605,\n",
       " 218646,\n",
       " 218647,\n",
       " 218648,\n",
       " 218673,\n",
       " 218700,\n",
       " 218764,\n",
       " 218839,\n",
       " 218841,\n",
       " 218845,\n",
       " 218861,\n",
       " 222602,\n",
       " 225212,\n",
       " 227839,\n",
       " 231337,\n",
       " 233954,\n",
       " 238821,\n",
       " 239872,\n",
       " 240303,\n",
       " 240422,\n",
       " 240567,\n",
       " 240631,\n",
       " 241270,\n",
       " 248254,\n",
       " 248778,\n",
       " 248834,\n",
       " 249662,\n",
       " 250416,\n",
       " 255936,\n",
       " 257512,\n",
       " 261892,\n",
       " 262145,\n",
       " 262158,\n",
       " 262170,\n",
       " 262171,\n",
       " 262179,\n",
       " 262183,\n",
       " 262186,\n",
       " 262190,\n",
       " 262200,\n",
       " 262203,\n",
       " 262206,\n",
       " 262220,\n",
       " 262259,\n",
       " 262260,\n",
       " 262272,\n",
       " 262282,\n",
       " 262284,\n",
       " 262298,\n",
       " 262306,\n",
       " 262307,\n",
       " 262328,\n",
       " 262334,\n",
       " 262335,\n",
       " 262358,\n",
       " 262368,\n",
       " 262388,\n",
       " 262392,\n",
       " 262398,\n",
       " 262413,\n",
       " 262414,\n",
       " 262441,\n",
       " 262453,\n",
       " 262462,\n",
       " 262464,\n",
       " 262476,\n",
       " 262491,\n",
       " 262494,\n",
       " 262507,\n",
       " 262518,\n",
       " 262520,\n",
       " 262528,\n",
       " 262537,\n",
       " 262540,\n",
       " 262543,\n",
       " 262544,\n",
       " 262548,\n",
       " 262549,\n",
       " 262551,\n",
       " 262553,\n",
       " 262560,\n",
       " 262578,\n",
       " 262587,\n",
       " 262598,\n",
       " 262602,\n",
       " 262618,\n",
       " 262622,\n",
       " 262661,\n",
       " 262669,\n",
       " 262682,\n",
       " 262687,\n",
       " 262689,\n",
       " 262690,\n",
       " 262691,\n",
       " 262703,\n",
       " 262704,\n",
       " 262706,\n",
       " 262709,\n",
       " 262714,\n",
       " 262717,\n",
       " 262723,\n",
       " 262726,\n",
       " 262746,\n",
       " 262747,\n",
       " 262751,\n",
       " 262769,\n",
       " 262776,\n",
       " 262785,\n",
       " 262844,\n",
       " 262847,\n",
       " 262850,\n",
       " 262860,\n",
       " 262861,\n",
       " 262868,\n",
       " 262883,\n",
       " 262892,\n",
       " 262896,\n",
       " 262900,\n",
       " 262908,\n",
       " 262916,\n",
       " 262931,\n",
       " 262951,\n",
       " 262953,\n",
       " 262961,\n",
       " 262966,\n",
       " 262975,\n",
       " 262978,\n",
       " 262979,\n",
       " 262990,\n",
       " 263005,\n",
       " 263007,\n",
       " 263030,\n",
       " 263041,\n",
       " 263042,\n",
       " 263046,\n",
       " 263083,\n",
       " 263093,\n",
       " 263097,\n",
       " 263100,\n",
       " 263110,\n",
       " 263111,\n",
       " 263134,\n",
       " 263137,\n",
       " 263146,\n",
       " 263188,\n",
       " 263195,\n",
       " 263203,\n",
       " 263207,\n",
       " 263211,\n",
       " 263228,\n",
       " 263260,\n",
       " 263263,\n",
       " 263264,\n",
       " 263269,\n",
       " 263270,\n",
       " 263273,\n",
       " 263274,\n",
       " 263275,\n",
       " 263277,\n",
       " 263301,\n",
       " 263310,\n",
       " 263319,\n",
       " 263327,\n",
       " 263332,\n",
       " 263357,\n",
       " 263376,\n",
       " 263380,\n",
       " 263382,\n",
       " 263387,\n",
       " 263399,\n",
       " 263404,\n",
       " 263405,\n",
       " 263406,\n",
       " 263417,\n",
       " 263419,\n",
       " 263433,\n",
       " 263439,\n",
       " 263445,\n",
       " 263452,\n",
       " 263453,\n",
       " 263455,\n",
       " 263481,\n",
       " 263502,\n",
       " 263511,\n",
       " 263515,\n",
       " 263573,\n",
       " 263575,\n",
       " 263581,\n",
       " 263603,\n",
       " 263608,\n",
       " 263617,\n",
       " 263619,\n",
       " 263622,\n",
       " 263638,\n",
       " 263649,\n",
       " 263676,\n",
       " 263684,\n",
       " 263698,\n",
       " 263709,\n",
       " 263713,\n",
       " 263726,\n",
       " 263733,\n",
       " 263734,\n",
       " 263743,\n",
       " 263758,\n",
       " 263761,\n",
       " 263763,\n",
       " 263766,\n",
       " 263778,\n",
       " 263783,\n",
       " 263809,\n",
       " 263822,\n",
       " 263825,\n",
       " 263844,\n",
       " 263858,\n",
       " 263872,\n",
       " 263874,\n",
       " 263875,\n",
       " 263877,\n",
       " 263882,\n",
       " 263895,\n",
       " 263900,\n",
       " 263923,\n",
       " 263967,\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids = []\n",
    "for x in questionsdf['image_id']:\n",
    "    if x not in image_ids:\n",
    "        image_ids.append(x)\n",
    "image_ids = np.array(image_ids) - 1\n",
    "sorted(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=r\"C:\\Users\\PIYUSH\\Desktop\\v2_mscoco_train2014_annotations.json\"\n",
    "with open(file,'r') as myfile:\n",
    "    data=myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=json.loads(data)['annotations']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vocab_answers(annotations, n_answers):\n",
    "    \"\"\"Make dictionary for top n answers and save them into text file.\"\"\"\n",
    "    answers = defaultdict(lambda: 0)\n",
    "    for annotation in annotations:\n",
    "            for answer in annotation['answers']:\n",
    "                word = answer['answer']\n",
    "                if re.search(r\"[^\\w\\s]\", word):\n",
    "                    continue\n",
    "                answers[word] += 1\n",
    "                \n",
    "    answers = sorted(answers, key=answers.get, reverse=True)\n",
    "    assert('<unk>' not in answers)\n",
    "    top_answers = ['<unk>'] + answers[:n_answers-1] # '-1' is due to '<unk>'\n",
    "    \n",
    "    with open('vocab_answers.txt', 'w') as f:\n",
    "        f.writelines([w+'\\n' for w in top_answers])\n",
    "\n",
    "    print('Make vocabulary for answers')\n",
    "    print('The number of total words of answers: %d' % len(answers))\n",
    "    print('Keep top %d answers into vocab' % n_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make vocabulary for answers\n",
      "The number of total words of answers: 135203\n",
      "Keep top 500 answers into vocab\n"
     ]
    }
   ],
   "source": [
    "make_vocab_answers(data1,500) ## this saves a text file in the same folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_featuresSENTENCE_SPLIT_REGEX = re.compile(r'(\\W+)')\n",
    "\n",
    "\n",
    "def tokenize(sentence):\n",
    "    tokens = SENTENCE_SPLIT_REGEX.split(sentence.lower())\n",
    "    tokens = [t.strip() for t in tokens if len(t.strip()) > 0]\n",
    "    return tokens\n",
    "def load_str_list(fname):\n",
    "    with open(fname) as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [l.strip() for l in lines]\n",
    "    return lines\n",
    "\n",
    "\n",
    "class VocabDict:\n",
    "\n",
    "    def __init__(self, vocab_file):\n",
    "        self.word_list = load_str_list(vocab_file)\n",
    "        self.word2idx_dict = {w:n_w for n_w, w in enumerate(self.word_list)}\n",
    "        self.vocab_size = len(self.word_list)\n",
    "        self.unk2idx = self.word2idx_dict['<unk>'] if '<unk>' in self.word2idx_dict else None\n",
    "    def idx2word(self, n_w):\n",
    "\n",
    "        return self.word_list[n_w]\n",
    "\n",
    "    def word2idx(self, w):\n",
    "        if w in self.word2idx_dict:\n",
    "            return self.word2idx_dict[w]\n",
    "        elif self.unk2idx is not None:\n",
    "            return self.unk2idx\n",
    "        else:\n",
    "            raise ValueError('word %s not in dictionary (while dictionary does not contain <unk>)' % w)\n",
    "\n",
    "    def tokenize_and_index(self, sentence):\n",
    "        inds = [self.word2idx(w) for w in tokenize(sentence)]\n",
    "\n",
    "        return inds\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_vocab = VocabDict('vocab_answers.txt')\n",
    "ans_vocab = ans_vocab.word2idx_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = np.array(list(ans_vocab.values()))\n",
    "answers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor=[]\n",
    "q_id=[]\n",
    "for i in range(questionsdf.shape[0]):\n",
    "    image_features = data[data['Image ID']==questionsdf.iloc[i]['image_id']]['Image Vector'].apply(lambda x: \n",
    "                               np.fromstring(\n",
    "                               x.replace('\\n','')\n",
    "                                .replace('[','')\n",
    "                                .replace(']','')\n",
    "                                .replace('  ',' '), sep=' '))\n",
    "    image=tf.convert_to_tensor(image_features, np.float32)\n",
    "    question=question_features[i]\n",
    "    if(question.shape[0]<11000):\n",
    "        x_tensor.append(concat_tensor(question,image[0]))\n",
    "        q_id.append(questionsdf.iloc[i][\"question_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_answer(question_id):\n",
    "    for answer in data1:\n",
    "        if answer['question_id']==question_id:\n",
    "            return answer['multiple_choice_answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_tensor)):\n",
    "    x_tensor[i]=x_tensor[i].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9685, 11600)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=np.array(x_tensor)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=np.zeros(shape=(len(x_tensor),500))\n",
    "for i in range(len(q_id)):\n",
    "    ans=unique_answer(q_id[i])\n",
    "    try:\n",
    "        key=ans_vocab[ans]\n",
    "        label[i][key]=1\n",
    "    except:\n",
    "        label[i][0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9685, 500)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label=np.array(label)\n",
    "label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9685, 11600)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1=200\n",
    "h2=200\n",
    "y=500\n",
    "model = Sequential()\n",
    "model.add(Dense(h1, input_dim=x_tensor[0].shape[0], activation='relu'))\n",
    "model.add(Dense(h2, activation='relu'))\n",
    "model.add(Dense(y, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 500)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=x_train[0:500]\n",
    "y=label[0:500]\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 500 samples\n",
      "500/500 [==============================] - 8s 15ms/sample - loss: 4.1657 - accuracy: 0.2900\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x,y, epochs=1, batch_size=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_image_features = np.load(\"Files/test_img_vectors.npy\")\n",
    "# test_image_features[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_questions = pd.read_csv('Files/questions_test.csv')\n",
    "# test_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lens = []\n",
    "# for quest in test_questions['question']:\n",
    "#     lens.append(len(quest))\n",
    "# test_question_length = np.array(lens)\n",
    "# test_question_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VQA_MODEL():\n",
    "    image_feature_size          = 512\n",
    "    word_feature_size           = 300\n",
    "    max_length_questions        = 30\n",
    "    number_of_dense_layers      = 3\n",
    "    number_of_hidden_units      = 1024\n",
    "    activation_function         = 'tanh'\n",
    "    dropout_pct                 = 0.5\n",
    "\n",
    "    # Image model\n",
    "    model_image = Sequential()\n",
    "    model_image.add(Reshape((image_feature_size,), input_shape=(image_feature_size,)))\n",
    "\n",
    "   # Language Model\n",
    "    model_language = Sequential()\n",
    "#     model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=True,input_shape=(max_length_questions, word_feature_size)))\n",
    "#     model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=True))  \n",
    "#     model_language.add(LSTM(number_of_hidden_units_LSTM, return_sequences=False))\n",
    "    #model_language.add(Reshape((word_feature_size,), input_shape=(word_feature_size,)))\n",
    "    model_language.add()\n",
    "    model.add(Dense(512))\n",
    "\n",
    "\n",
    "    # combined model\n",
    "    x = Concatenate()([model_language.output, model_image.output])\n",
    "\n",
    "    for _ in range(number_of_dense_layers):\n",
    "        x = Dense(number_of_hidden_units, kernel_initializer='uniform', activation= activation_function)(x)\n",
    "        x = Dropout(dropout_pct)(x)\n",
    "        \n",
    "    x = Dense(50, activation='softmax')(x)\n",
    "    \n",
    "    model = Model(inputs = [model_language.input, model_image.input], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=47143, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 1],\n",
       "       [2, 2]])>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "paddings = tf.constant([[1, 1,], [2, 2]])\n",
    "paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# def get_arguments():\n",
    "\n",
    "#     parser = argparse.ArgumentParser()\n",
    "#     # model\n",
    "#     parser.add_argument('-model'                  , type=str   , default='simple_mlp')\n",
    "#     parser.add_argument('-num_hidden_units_mlp'   , type=int   , default=1024)\n",
    "#     parser.add_argument('-num_hidden_units_lstm'  , type=int   , default=512)\n",
    "#     parser.add_argument('-num_hidden_layers_mlp'  , type=int   , default=3)\n",
    "#     parser.add_argument('-num_hidden_layers_lstm' , type=int   , default=1)\n",
    "#     parser.add_argument('-dropout'                , type=float , default=0.5)\n",
    "#     parser.add_argument('-activation_1'           , type=str   , default='tanh')\n",
    "#     parser.add_argument('-activation_2'           , type=str   , default='relu')\n",
    "\n",
    "#     # training\n",
    "#     parser.add_argument('-seed'                   , type=int   , default=1337)\n",
    "#     parser.add_argument('-optimizer'              , type=str   , default='rmsprop')\n",
    "#     parser.add_argument('-nb_epoch'               , type=int   , default=300)\n",
    "#     parser.add_argument('-nb_iter'                , type=int   , default=200000)\n",
    "#     parser.add_argument('-model_save_interval'    , type=int   , default=19)\n",
    "#     parser.add_argument('-batch_size'             , type=int   , default=128)\n",
    "\n",
    "    # language features\n",
    "#     parser.add_argument('-word_vector'            , type=str   , default='glove')\n",
    "#     parser.add_argument('-word_emb_dim'           , type=int   , default=300)\n",
    "#     parser.add_argument('-vocabulary_size'        , type=int   , default=12603)\n",
    "#     parser.add_argument('-max_ques_length'        , type=int   , default=26)\n",
    "#     parser.add_argument('-data_type'              , type=str   , default='TRAIN')\n",
    "\n",
    "    # image features\n",
    "#     parser.add_argument('-img_vec_dim'            , type=int   , default=2048)\n",
    "#     parser.add_argument('-img_features'           , type=str   , default='resnet')\n",
    "#     parser.add_argument('-img_normalize'          , type=int   , default=0)\n",
    "\n",
    "    # evaluations\n",
    "#     parser.add_argument('-nb_classes'             , type=int   , default=1000)\n",
    "#     parser.add_argument('-class_activation'       , type=str   , default='softmax')\n",
    "#     parser.add_argument('-loss'                   , type=str   , default='categorical_crossentropy')\n",
    "#     parser.add_argument('-save_folder'            , type=str   , default='')\n",
    "\n",
    "#     # data\n",
    "#     parser.add_argument('-ans_file'               , type=str   , default='data/val_all_answers_dict.json')\n",
    "#     parser.add_argument('-input_json'             , type=str   , default='data/data_prepro.json')\n",
    "#     parser.add_argument('-input_img_h5'           , type=str   , default='data/data_img.h5')\n",
    "#     parser.add_argument('-input_ques_h5'          , type=str   , default='data/data_prepro.h5')\n",
    "\n",
    "\n",
    "#     return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'simple_mlp'\n",
    "num_hidden_units_mlp = 1024\n",
    "num_hidden_units_lstm = 512\n",
    "num_hidden_layers_mlp = 3\n",
    "num_hidden_layers_lstm = 1\n",
    "dropout = 0.5\n",
    "activation_1 = 'tanh'\n",
    "activation_2 = 'relu'\n",
    "seed = 1337\n",
    "optimizer = 'rmsprop'\n",
    "nb_epoch = 300\n",
    "nb_iter = 200000\n",
    "model_save_interval = 19\n",
    "batch_size = 128\n",
    "word_vector = 'glove'\n",
    "word_emb_dim = 300\n",
    "vocabulary_size = 12603\n",
    "max_ques_length = 26\n",
    "data_type = 'TRAIN'\n",
    "img_vec_dim = 2048\n",
    "img_features = 'resnet'\n",
    "img_normalize = 0\n",
    "nb_classes = 500\n",
    "class_activation = 'softmax'\n",
    "loss = 'categorical_crossentropy'\n",
    "save_folder = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data():\n",
    "\n",
    "    train_data = {}\n",
    "    print('loading image feature...')\n",
    "    #img_feature = np.load('Files/img_vectors_sample.npy')\n",
    "    train_data['question'] = question_features \n",
    "    train_data['answers'] = answers\n",
    "\n",
    "    print('Normalizing image feature')\n",
    "    if img_normalize:\n",
    "        tem = np.sqrt(np.sum(np.multiply(img_feature, img_feature)))\n",
    "        img_feature = np.divide(img_feature, np.tile(tem,(1,img_vec_dim)))\n",
    "\n",
    "    return img_feature, train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_data_test(args):\n",
    "#     dataset = {}\n",
    "#     test_data = {}\n",
    "#     # load json file\n",
    "#     print('loading json file...')\n",
    "#     with open(args.input_json) as data_file:\n",
    "#         data = json.load(data_file)\n",
    "#     for key in data.keys():\n",
    "#         dataset[key] = data[key]\n",
    "\n",
    "#     # load image feature\n",
    "#     print('loading image feature...')\n",
    "#     img_feature = np.load('img_vectors_sample.npy')\n",
    "    \n",
    "#     # load h5 file\n",
    "#     print('loading h5 file...')\n",
    "#     with h5py.File(args.input_ques_h5,'r') as hf:\n",
    "#         # total number of training data is 215375\n",
    "#         # question is (26, )\n",
    "#         tem = hf.get('ques_test')\n",
    "#         test_data['question'] = test_questions\n",
    "#         # max length is 23\n",
    "#         tem = hf.get('ques_length_test')\n",
    "#         test_data['length_q'] = np.array(tem)\n",
    "#         # total 82460 img\n",
    "#         # -----1~82460-----\n",
    "#         tem = hf.get('img_pos_test')\n",
    "#         # convert into 0~82459\n",
    "#         test_data['img_list'] = np.array(tem)-1\n",
    "#         # quiestion id\n",
    "#         tem = hf.get('question_id_test')\n",
    "#         test_data['ques_id'] = np.array(tem)\n",
    "#     # MC_answer_test\n",
    "#     tem = hf.get('MC_ans_test')\n",
    "#     test_data['MC_ans_test'] = np.array(tem)\n",
    "\n",
    "#     print('Normalizing image feature')\n",
    "#     if img_norm:\n",
    "#         tem =  np.sqrt(np.sum(np.multiply(img_feature, img_feature)))\n",
    "#         img_feature = np.divide(img_feature, np.tile(tem,(1,args.img_vec_dim)))\n",
    "\n",
    "\n",
    "#     # make sure the ans_file is provided\n",
    "#     nb_data_test = len(test_data[u'question'])\n",
    "#     val_all_answers_dict = json.load(open(args.ans_file))\n",
    "#     val_answers = np.zeros(nb_data_test, dtype=np.int32)\n",
    "\n",
    "#     ans_to_ix = {v: k for k, v in dataset[u'ix_to_ans'].items()}\n",
    "#     count_of_not_found = 0\n",
    "#     for i in xrange(nb_data_test):\n",
    "#         qid = test_data[u'ques_id'][i]\n",
    "#         try : \n",
    "#             val_ans_ix =int(ans_to_ix[most_common(val_all_answers_dict[str(qid)])]) -1\n",
    "#         except KeyError:\n",
    "#             count_of_not_found += 1\n",
    "#             val_ans_ix = 480\n",
    "#         val_answers[i] = val_ans_ix\n",
    "#     print(\"Beware: \" + str(count_of_not_found) + \" number of val answers are not really correct\")\n",
    "\n",
    "#     return img_feature, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading image feature...\n",
      "Normalizing image feature\n",
      "1913\n"
     ]
    }
   ],
   "source": [
    "train_img_feature, train_data = get_train_data()\n",
    "# test_img_feature,  test_data, val_answers = get_test_data(args)\n",
    "\n",
    "train_X = [train_data[u'question'], train_img_feature]\n",
    "# train_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\n",
    "# train_Y = np_utils.to_categorical(train_data[u'answers'], nb_classes)\n",
    "\n",
    "# # test_X = [test_data[u'question'], test_img_feature]\n",
    "# # test_Y = np_utils.to_categorical(val_answers, args.nb_classes)\n",
    "\n",
    "\n",
    "# # model_name = importlib.import_module(\"models.\"+model)\n",
    "\n",
    "# model = VQA_MODEL()\n",
    "# model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "# model.build(train_img_feature.shape)\n",
    "# model.summary() # prints model layers with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected reshape_2_input to have shape (300,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-b60c079b0075>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# history = model.fit(train_X, train_Y, batch_size = args.batch_size, nb_epoch=args.nb_epoch, validation_data=(test_X, test_Y))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1152\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1153\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1154\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1156\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    143\u001b[0m                             \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m                             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m                             str(data_shape))\n\u001b[0m\u001b[1;32m    146\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected reshape_2_input to have shape (300,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_X, train_Y, batch_size = batch_size, epochs=nb_epoch)\n",
    "# history = model.fit(train_X, train_Y, batch_size = args.batch_size, nb_epoch=args.nb_epoch, validation_data=(test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"./VQA_MODEL_WEIGHTS.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VQA_weights_file_name   = \"./VQA_MODEL_WEIGHTS.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_VQA_model(VQA_weights_file_name):\n",
    "    ''' Given the VQA model and its weights, compiles and returns the model '''\n",
    "\n",
    "    from models.VQA.VQA import VQA_MODEL\n",
    "    vqa_model = VQA_MODEL()\n",
    "    vqa_model.load_weights(VQA_weights_file_name)\n",
    "\n",
    "    vqa_model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return vqa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_model = get_VQA_model(VQA_weights_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_model.predict([question_features, image_features])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
